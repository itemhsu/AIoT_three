{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiVBUpuHXEtw"
      },
      "source": [
        "# CNN to classify Cifar-10 dataset (Images)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gt0KwMrt0I0n"
      },
      "source": [
        "So far, we saw how to build a Dense Neural Network (DNN) that classified images of digits (MNIST) or even fashion images (Fashion-MNIST). Here we will instead, recognize the 10 classes of CIFAR ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' and 'truck'). There are some key differences between these two image datasets that we need to take into account.\n",
        "\n",
        "First, while MNIST were 28x28 monochrome images (1 color channel), CIFAR is 32x32 color images (3 color channels).\n",
        "\n",
        "Second, MNIST images are simple, containing just the object centered in the image, with no background. Conversely, CIFAR ones are not centered and can have the object with a background, such as airplanes that might have a cloudy sky behind them! Those differences are the main reason to use a CNN instead of a DNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WnUJoAL1pc2"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OQ_tVTaU3oo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPogyxx5zt5J"
      },
      "source": [
        "## Import and Inspect Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNoaUYniNr5M"
      },
      "source": [
        "Cifar-10 repository: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiwcGwmLJVIc"
      },
      "outputs": [],
      "source": [
        "cifar10 = tf.keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3x39c36sJ60N"
      },
      "outputs": [],
      "source": [
        "print(train_images.shape, train_labels.shape)\n",
        "print(test_images.shape, test_labels.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzWNjl4JjiKU"
      },
      "source": [
        "- The image data shape is: `(#images, img_heigth, img_width, #channels)`, where channels are in RGB format (red, green, blue).\n",
        "- The labels shape is `(#images, label)`, where label goes from 0 to 9.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vopv6HetVj8y"
      },
      "outputs": [],
      "source": [
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chqX49HIlW5c"
      },
      "outputs": [],
      "source": [
        "plt.imshow(train_images[1]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APDXWEtgKVfT"
      },
      "outputs": [],
      "source": [
        "train_labels[1][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CIFAR labels happen to be arrays, which is why you need the extra index."
      ],
      "metadata": {
        "id": "hUyUOC3j3A9D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oj0W6uIQxDm"
      },
      "outputs": [],
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3O7Gy8-HU_5"
      },
      "outputs": [],
      "source": [
        "class_names[9] # The List's index is the label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMfskv-rpXv-"
      },
      "outputs": [],
      "source": [
        "idx = train_labels[1][0]\n",
        "class_names[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTRiPf7QKJWh"
      },
      "outputs": [],
      "source": [
        "print(\"\\t\", class_names[train_labels[1][0]])\n",
        "plt.imshow(train_images[1])\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tubjj9IeLLWp"
      },
      "outputs": [],
      "source": [
        " def plot_train_img(img, size=2):\n",
        "    label = train_labels[img][0]\n",
        "    plt.figure(figsize=(size,size))\n",
        "    print(\"Label {} - {}\".format(label, class_names[label]))\n",
        "    plt.imshow(train_images[img])\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw_E_Jp4M1fK"
      },
      "outputs": [],
      "source": [
        "plot_train_img(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIww0M0YqFe3"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyjP8DsPp-rj"
      },
      "source": [
        "Note that images are in color, not centered and with different backgrounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuPx6Fpg2-ER"
      },
      "source": [
        "## Preprocessing dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDC5qAy9lxHI"
      },
      "outputs": [],
      "source": [
        "test_images.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zr1V-RwhJ0dC"
      },
      "outputs": [],
      "source": [
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx8cqGbeWHB2"
      },
      "outputs": [],
      "source": [
        "test_images.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8FbgbiaYpNV"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_labels[:5000]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t5JF1P-Zm3f"
      },
      "outputs": [],
      "source": [
        "val_images = train_images[:5000]\n",
        "val_labels = train_labels[:5000]\n",
        "print(val_images.shape, val_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxWvJq-BaB8s"
      },
      "outputs": [],
      "source": [
        "train_images = train_images[5000:]\n",
        "train_labels = train_labels[5000:]\n",
        "print(train_images.shape, train_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peWqyBAyaw5X"
      },
      "outputs": [],
      "source": [
        "plt.hist(train_labels, alpha=0.5)\n",
        "plt.hist(val_labels, alpha=0.5)\n",
        "plt.hist(test_labels, alpha=0.5);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsvJeVm6_3VB"
      },
      "source": [
        "## Create Model Architecture and Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF73ofMGqt3p"
      },
      "source": [
        "On [Convolution layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D),\n",
        "- strides is an integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Default (1,1).\n",
        "- padding: one of \"valid\" or \"same\" (case-insensitive). Default = 'valid'.\n",
        "  - \"valid\" means no padding.  \n",
        "  - \"same\" results in padding with zeros evenly\n",
        "to the left/right or up/down of the input such that output has the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FTRI--UM5fK"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "\n",
        "model.add(Conv2D(\n",
        "    filters=32,\n",
        "    kernel_size=(3,3),\n",
        "    activation='relu',\n",
        "    input_shape=(32, 32, 3))\n",
        ")\n",
        "model.add(MaxPool2D(2, 2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), activation='relu'))\n",
        "model.add(MaxPool2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxq2JU7dVIEX"
      },
      "outputs": [],
      "source": [
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "OPTIMIZER = 'adam'\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=OPTIMIZER,\n",
        "              loss=LOSS,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utH8U6ud44cY"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjAJ1xXz4pPE"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 10\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1QKpQHFoVJI"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "history = model.fit(train_images,\n",
        "                    train_labels,\n",
        "                    epochs=NUM_EPOCHS,\n",
        "                    validation_data=(val_images, val_labels),\n",
        "                    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2VQKFylodzi"
      },
      "outputs": [],
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "#plt.xlim([0,NUM_EPOCHS])\n",
        "plt.ylim([0.4,1.0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9KzYRI05g_M"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3eLN-JVvduA"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTW4IkOrwAAl"
      },
      "source": [
        "**Accuracy**\n",
        "- Train: 85% - 90%;\n",
        "- Validation: 68%-70%\n",
        "- Test: 66%-68%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD_r_qxxwF1M"
      },
      "outputs": [],
      "source": [
        "predictions = np.argmax(model.predict(test_images), axis=-1)\n",
        "predictions.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWEA_ALv51P3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hDUe5uq6PoX"
      },
      "outputs": [],
      "source": [
        "print(classification_report(test_labels, predictions, target_names=class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCwieAWB6yAh"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(test_labels,predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOkcOflwgoCR"
      },
      "outputs": [],
      "source": [
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3QJQ3ER7Btb"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.heatmap(confusion_matrix(test_labels,predictions), cmap='Blues', annot=True, fmt='g');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhL-fPKh7ryx"
      },
      "source": [
        "## Testing Model (Predicting)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8q8RbCU-i5g"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test_images[15]);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GDFWkXG_H6Y"
      },
      "outputs": [],
      "source": [
        "test_labels[15][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LshDRY7-8Iq"
      },
      "outputs": [],
      "source": [
        "class_names[8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eE6NF_wV-YfZ"
      },
      "outputs": [],
      "source": [
        "test_images[15].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UGml1PgAYxS"
      },
      "source": [
        "The input Tensor shape should be: (num_images, width, height, color_channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-bF-5tY_-RT"
      },
      "outputs": [],
      "source": [
        "my_image = test_images[15]\n",
        "my_image = my_image.reshape(1,32,32,3)\n",
        "my_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGUt18nZ_WN8"
      },
      "outputs": [],
      "source": [
        "img_pred = np.argmax(model.predict(my_image))\n",
        "class_names[img_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OofdGSufEIHT"
      },
      "outputs": [],
      "source": [
        "img_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF1DSVq1DGAV"
      },
      "outputs": [],
      "source": [
        "pred_prob = model.predict(my_image)[0][img_pred]\n",
        "pred_prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kojDXuqRBodN"
      },
      "outputs": [],
      "source": [
        "def img_pred(img, size=4):\n",
        "    label = test_labels[img][0]\n",
        "    my_image = test_images[img]\n",
        "    plt.figure(figsize=(size,size))\n",
        "    plt.imshow(my_image)\n",
        "    my_image = my_image.reshape(1,32,32,3)\n",
        "    img_pred = np.argmax(model.predict(my_image))\n",
        "    pred_label = class_names[img_pred]\n",
        "    pred_prob = model.predict(my_image)[0][img_pred]\n",
        "    print(\" Label {} <=> Pred: {} with prob {:.2}\".format(\n",
        "        class_names[label],\n",
        "        pred_label,\n",
        "        pred_prob))\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E11rQVtPCg5x"
      },
      "outputs": [],
      "source": [
        "img_pred(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kIEAJ6KE86i"
      },
      "outputs": [],
      "source": [
        "for i in range (5):\n",
        "  img_pred(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhnnOt7bgXGg"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take now some time and read more about saving models. Things have changed a bit recently. Read here:\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/serialization_and_saving\n",
        "\n",
        "You can save a model with model.save() or keras.models.save_model() (which is equivalent).\n",
        "\n",
        "You can load it back with keras.models.load_model().\n",
        "\n",
        "The recommended format is the \"Keras v3\" format, which uses the .keras extension.\n",
        "\n",
        "There are, however, **two legacy formats** that are available: the **TensorFlow SavedModel** format and the older **Keras H5** format.\n",
        "\n",
        "You can switch to the SavedModel format by:\n",
        "\n",
        "    Passing save_format='tf' to save()\n",
        "    Passing a filename without an extension\n",
        "\n",
        "You can switch to the H5 format by:\n",
        "\n",
        "    Passing save_format='h5' to save()\n",
        "    Passing a filename that ends in .h5\n"
      ],
      "metadata": {
        "id": "dQDz_Dwx5nk2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ty0qLh9ngk2-"
      },
      "outputs": [],
      "source": [
        "!pwd # Linux command, shows where we are in CoLab's folders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Save model as in **Keras V3** format:"
      ],
      "metadata": {
        "id": "177n8U_FBs4i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vN8z8vpgaup"
      },
      "outputs": [],
      "source": [
        "model.save('cifar_10_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/cifar_10_model.keras"
      ],
      "metadata": {
        "id": "Gpqxw7jzCHGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Save model as in **TensorFlow SavedModel** format:"
      ],
      "metadata": {
        "id": "ara2uv9YB3LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('cifar_10_model')"
      ],
      "metadata": {
        "id": "604FPnHQB-dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/cifar_10_model\n",
        "!du -sh /content/cifar_10_model"
      ],
      "metadata": {
        "id": "iozHasSQYtde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Save model as in **Keras H5** format:"
      ],
      "metadata": {
        "id": "RrP6g5P7CMbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('cifar_10_model.h5')"
      ],
      "metadata": {
        "id": "K3hbyt9S5CAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du -sh /content/cifar_10_model.h5"
      ],
      "metadata": {
        "id": "Xo2-E_hqCd7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdEONve0j0ht"
      },
      "source": [
        "NOTE 1:\n",
        "\n",
        "If you save your model like just discussed and would like to have them for later - before closing your Colab session **you should download them to your own computer**. Later, you will then only need to upload the saved model back into your othrr Colab session and start from it, without the need to retrain!\n",
        "\n",
        "NOTE 2:\n",
        "\n",
        "Use [Netron](https://netron.app) to visualize the model, hyperparameters, tensor shapes, etc. Netron is a viewer for neural network, deep learning and machine learning models (See [GitHub](https://github.com/lutzroeder/netron) for instructions about instalation in your desktop)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0Fksxpps31l"
      },
      "source": [
        "# Converting to TensorFlow Lite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHkvBxKKffkI"
      },
      "source": [
        "You can convert the TF trained model using:  `TFLiteConverter.from_keras_model(model)`\n",
        "\n",
        "Read more about converting models here:\n",
        "\n",
        "https://www.tensorflow.org/lite/models/convert/convert_models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8T6QFv7fdAs"
      },
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ogba6QEtfrzw"
      },
      "source": [
        "Alternativally, **we can do that from a previously saved model, to avoid doing retraining**:\n",
        "\n",
        "1) From a Keras V3 (recommended) saved model: `tf.lite.TFLiteConverter.from_keras_model(model_cifar10.keras)`\n",
        "\n",
        "2) From a TF saved model: `tf.lite.TFLiteConverter.from_saved_model(model_cifar10)`\n",
        "\n",
        "3) From a Keras H5 saved model: `tf.lite.TFLiteConverter.from_keras_model(model_cifar10.h5)`\n",
        "\n",
        "Uncomment and run any of these options below - if you want to do that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH5SnzjAtUcv"
      },
      "outputs": [],
      "source": [
        "# 1) From Keras V3\n",
        "#model2_path = '/content/cifar_10_model.keras'\n",
        "#model2 = tf.keras.models.load_model(model2_path)\n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model2)\n",
        "\n",
        "# 2) From TF saved\n",
        "#model3_path = '/content/cifar_10_model'\n",
        "#converter = tf.lite.TFLiteConverter.from_saved_model(model3_path)\n",
        "\n",
        "# 3) From Keras H5 saved\n",
        "#model4_path = '/content/cifar_10_model.h5'\n",
        "#model4 = tf.keras.models.load_model(model4_path)\n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GdiCDhvt3Ry"
      },
      "outputs": [],
      "source": [
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN53gyEtt-cK"
      },
      "outputs": [],
      "source": [
        "# Save .tflite model\n",
        "tflite_model_size = open(\"/content/cifar10_model.tflite\",\"wb\").write(tflite_model)\n",
        "print(\"TFLite model is {:,} bytes\".format(tflite_model_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6alsly58mggB"
      },
      "source": [
        "## Dynamic Range Quantization - DEFAULT Options\n",
        "The simplest form of **post-training** quantization statically **quantizes only the weights from floating point to integer**, which has 8-bits of precision. Dynamic Range Quantization often provides a good balance between model size reduction and accuracy preservation. Read more at:\n",
        "\n",
        "https://www.tensorflow.org/lite/performance/post_training_quantization\n",
        "\n",
        "`optimizations` is an xperimental flag, subject to change. Set of optimizations to apply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a set of values of type tf.lite.Optimize). Read more at:\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter\n",
        "\n",
        "DEFAULT The default optimization strategy that enables post-training quantization. The type of post-training quantization that will be used is dependent on the other converter options supplied. Read more at:\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/lite/Optimize\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33zb5BSYwqe-"
      },
      "outputs": [],
      "source": [
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model_quant = converter.convert()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZJs4AkZnJDI"
      },
      "outputs": [],
      "source": [
        "# Save .tflite model and print its size on disk:\n",
        "tflite_model_quant_size = open(\"/content/cifar10_model_quant.tflite\",\"wb\").write(tflite_model_quant)\n",
        "print(\"TFLite Quantized model (with DEFAULT optimizations) is {:,} bytes\".format(tflite_model_quant_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPzHZELar3Os"
      },
      "source": [
        "NOTE:\n",
        "\n",
        "Use [Netron](https://netron.app) to visualize the quantized model. Pay attention that now the weights are int8."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eg01uNRwxkq"
      },
      "source": [
        "## Testing TFLite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreter is an interface for running TensorFlow Lite models."
      ],
      "metadata": {
        "id": "hgS1PVpEcwFo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FKrv21y1G4w"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(\"/content/cifar10_model_quant.tflite\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHZm4ztQ1yku"
      },
      "outputs": [],
      "source": [
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwBI7x3D2HSp"
      },
      "outputs": [],
      "source": [
        "input_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo9Uci3O2YJE"
      },
      "outputs": [],
      "source": [
        "output_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQZ25ePb286y"
      },
      "outputs": [],
      "source": [
        "def set_input_tensor(interpreter, image):\n",
        "    tensor_index = interpreter.get_input_details()[0]['index']\n",
        "    input_tensor = interpreter.tensor(tensor_index)()[0]\n",
        "    input_tensor[:, :] = image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qwGTmnH7BSO"
      },
      "outputs": [],
      "source": [
        "image = test_images[0]\n",
        "plt.imshow(image);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmMayAqu619U"
      },
      "outputs": [],
      "source": [
        "set_input_tensor(interpreter, image)\n",
        "interpreter.invoke()\n",
        "output_details = interpreter.get_output_details()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl1ah6LnHCzp"
      },
      "outputs": [],
      "source": [
        "interpreter.get_tensor(output_details['index'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The squeeze() function in NumPy is used to remove an axis of length 1 from an input array. This will remove one set of [] from above array. Read some more info at:\n",
        "\n",
        "https://www.geeksforgeeks.org/numpy-squeeze-in-python/"
      ],
      "metadata": {
        "id": "6Zjj8ER98Azi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NI5jYogHGob"
      },
      "outputs": [],
      "source": [
        "np.squeeze(interpreter.get_tensor(output_details['index']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFI3HIk7i8Q"
      },
      "outputs": [],
      "source": [
        "output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8RlpEe08UVl"
      },
      "outputs": [],
      "source": [
        "img_pred = np.argmax(output)\n",
        "class_names[img_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E1ni5ALF009"
      },
      "outputs": [],
      "source": [
        "img_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pe897NEJFwcE"
      },
      "outputs": [],
      "source": [
        "output[img_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBIum-db-dkQ"
      },
      "outputs": [],
      "source": [
        "def classify_image(image):\n",
        "    set_input_tensor(interpreter, image)\n",
        "    interpreter.invoke()\n",
        "    output_details = interpreter.get_output_details()[0]\n",
        "    output = np.squeeze(interpreter.get_tensor(output_details['index']))\n",
        "    img_pred = np.argmax(output)\n",
        "    pred_label = class_names[img_pred]\n",
        "    pred_prob = output[img_pred]\n",
        "    plt.imshow(image)\n",
        "    print(\" Pred: {} with prob {:.2}\".format(pred_label, pred_prob))\n",
        "    plt.grid(False)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itLar4xF_m_G"
      },
      "outputs": [],
      "source": [
        "classify_image(test_images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kflk_u-aBhqe"
      },
      "outputs": [],
      "source": [
        "for i in range (5):\n",
        "  classify_image(test_images[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2NCPuTd07fr"
      },
      "source": [
        "# TensorFlow Lite Micro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PYjt32yBEVU"
      },
      "source": [
        "### Generate a TensorFlow Lite for Microcontrollers Model\n",
        "To convert the **TensorFlow Lite (TFL)** quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers on MCUs, we simply need to use the ```xxd``` tool to convert the ```.tflite``` file into a ```.cc``` file. But, first, install xxd:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-L6O8zj7as9"
      },
      "outputs": [],
      "source": [
        "!apt-get update && apt-get -qq install xxd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krTZ7tFDBiK1"
      },
      "source": [
        "Now, convert and save the .cc converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFFrKX3_-Vvg"
      },
      "outputs": [],
      "source": [
        "MODEL_TFLITE = 'cifar10_model_quant.tflite'\n",
        "MODEL_TFLITE_MICRO = 'cifar10_model_quant.cc'\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38A1ZGaV_xDt"
      },
      "source": [
        "If you'd like to download your model for safekeeping:\n",
        "1. On the left of the UI click on the folder icon\n",
        "2. Click on the three dots to the right of the ```cifar10_model_quant.cc``` file and select download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iof2YquSAH7S"
      },
      "outputs": [],
      "source": [
        "!cat {MODEL_TFLITE_MICRO}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}